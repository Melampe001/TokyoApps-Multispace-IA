name: Data Pipeline - ETL

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      start_date:
        description: 'Start date (YYYY-MM-DD) - optional'
        required: false
        type: string
      end_date:
        description: 'End date (YYYY-MM-DD) - optional'
        required: false
        type: string
      table:
        description: 'Specific table to export (leave empty for all)'
        required: false
        type: string

env:
  PYTHON_VERSION: '3.11'
  AWS_REGION: 'us-east-1'

jobs:
  etl-export:
    name: Export PostgreSQL to S3
    runs-on: ubuntu-latest
    
    permissions:
      id-token: write
      contents: read
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r python/etl/requirements.txt

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ETL_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Run ETL export
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          S3_DATA_LAKE_BUCKET: ${{ secrets.S3_DATA_LAKE_BUCKET }}
          S3_ATHENA_RESULTS_BUCKET: ${{ secrets.S3_ATHENA_RESULTS_BUCKET }}
          ATHENA_DATABASE: ${{ secrets.ATHENA_DATABASE }}
          ATHENA_WORKGROUP: ${{ secrets.ATHENA_WORKGROUP }}
        run: |
          cd python/etl
          
          # Build command with optional parameters
          CMD="python export_to_s3.py"
          
          if [ -n "${{ github.event.inputs.table }}" ]; then
            CMD="$CMD --table ${{ github.event.inputs.table }}"
          fi
          
          if [ -n "${{ github.event.inputs.start_date }}" ]; then
            CMD="$CMD --start-date ${{ github.event.inputs.start_date }}"
          fi
          
          if [ -n "${{ github.event.inputs.end_date }}" ]; then
            CMD="$CMD --end-date ${{ github.event.inputs.end_date }}"
          fi
          
          echo "Running: $CMD"
          $CMD

      - name: Update Athena partitions
        env:
          ATHENA_DATABASE: ${{ secrets.ATHENA_DATABASE }}
          ATHENA_WORKGROUP: ${{ secrets.ATHENA_WORKGROUP }}
          S3_ATHENA_RESULTS_BUCKET: ${{ secrets.S3_ATHENA_RESULTS_BUCKET }}
        run: |
          cd python/etl
          python athena_setup.py

      - name: Validate data export
        run: |
          echo "ETL pipeline completed successfully"
          echo "Exported data to S3 data lake"

      - name: Notify on failure
        if: failure()
        run: |
          echo "::error::ETL pipeline failed. Check logs for details."
          # Add Slack/Email notification here if needed

  validate-data:
    name: Validate exported data
    runs-on: ubuntu-latest
    needs: etl-export
    
    permissions:
      id-token: write
      contents: read
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ETL_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install AWS CLI
        run: |
          pip install awscli

      - name: Check S3 data
        env:
          S3_DATA_LAKE_BUCKET: ${{ secrets.S3_DATA_LAKE_BUCKET }}
        run: |
          echo "Checking data in S3 bucket..."
          aws s3 ls s3://$S3_DATA_LAKE_BUCKET/ --recursive | tail -n 20

      - name: Generate validation report
        run: |
          echo "## ETL Validation Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Data exported successfully" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Partitions updated in Athena" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Files verified in S3" >> $GITHUB_STEP_SUMMARY

  start-crawler:
    name: Start Glue Crawler
    runs-on: ubuntu-latest
    needs: validate-data
    
    permissions:
      id-token: write
      contents: read
    
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ETL_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Start Glue Crawler
        env:
          GLUE_CRAWLER_NAME: ${{ secrets.GLUE_CRAWLER_NAME }}
        run: |
          echo "Starting Glue crawler: $GLUE_CRAWLER_NAME"
          aws glue start-crawler --name $GLUE_CRAWLER_NAME || echo "Crawler already running"

      - name: Summary
        run: |
          echo "## ETL Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ All steps completed successfully" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "- Monitor Glue crawler execution" >> $GITHUB_STEP_SUMMARY
          echo "- Verify new partitions in Athena" >> $GITHUB_STEP_SUMMARY
          echo "- Run analytics queries" >> $GITHUB_STEP_SUMMARY
